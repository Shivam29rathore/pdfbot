{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/shivam29rathore/optimisedcustomchatbot?scriptVersionId=144340573\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"!pip install pdfplumber\n!pip install transformers\n!pip install langchain sentence-transformers\n!pip install tiktoken\n!pip install chromadb\n!pip install constants\n!pip install accelerate\n!pip install -i https://test.pypi.org/simple/ bitsandbytes\n!pip install bitsandbytes\n\n","metadata":{"execution":{"iopub.status.busy":"2023-09-25T07:47:28.006777Z","iopub.execute_input":"2023-09-25T07:47:28.00727Z","iopub.status.idle":"2023-09-25T07:48:28.279367Z","shell.execute_reply.started":"2023-09-25T07:47:28.00723Z","shell.execute_reply":"2023-09-25T07:48:28.278014Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from langchain.embeddings import HuggingFaceEmbeddings\nfrom langchain.document_loaders.pdf import PDFPlumberLoader\nfrom langchain.text_splitter import CharacterTextSplitter,TokenTextSplitter\nfrom sentence_transformers import SentenceTransformer, CrossEncoder, util\nfrom transformers import AutoTokenizer, pipeline\nfrom langchain.vectorstores import Chroma\nfrom langchain.llms import HuggingFacePipeline\nfrom langchain.chains import RetrievalQA\nfrom langchain.prompts import PromptTemplate\n\nimport torch","metadata":{"execution":{"iopub.status.busy":"2023-09-25T07:48:28.281535Z","iopub.execute_input":"2023-09-25T07:48:28.28191Z","iopub.status.idle":"2023-09-25T07:48:28.288956Z","shell.execute_reply.started":"2023-09-25T07:48:28.281873Z","shell.execute_reply":"2023-09-25T07:48:28.288226Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Embedding Opensource models available\n\nEMB_INSTRUCTOR_XL = \"hkunlp/instructor-xl\"\nEMB_SBERT_MPNET_BASE = \"sentence-transformers/all-mpnet-base-v2\"","metadata":{"execution":{"iopub.status.busy":"2023-09-25T07:48:34.124512Z","iopub.execute_input":"2023-09-25T07:48:34.124938Z","iopub.status.idle":"2023-09-25T07:48:34.129707Z","shell.execute_reply.started":"2023-09-25T07:48:34.124905Z","shell.execute_reply":"2023-09-25T07:48:34.128903Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# High performing Opensource LLMs available\n\nLLM_FLAN_T5_XXL = \"google/flan-t5-xxl\"\nLLM_FLAN_T5_XL = \"google/flan-t5-xl\"\nLLM_FASTCHAT_T5_XL = \"lmsys/fastchat-t5-3b-v1.0\"\nLLM_FLAN_T5_SMALL = \"google/flan-t5-small\"\nLLM_FLAN_T5_BASE = \"google/flan-t5-base\"\nLLM_FLAN_T5_LARGE = \"google/flan-t5-large\"\nLLM_FALCON_SMALL = \"tiiuae/falcon-7b-instruct\"","metadata":{"execution":{"iopub.status.busy":"2023-09-25T07:48:36.176983Z","iopub.execute_input":"2023-09-25T07:48:36.177465Z","iopub.status.idle":"2023-09-25T07:48:36.183614Z","shell.execute_reply.started":"2023-09-25T07:48:36.177426Z","shell.execute_reply":"2023-09-25T07:48:36.182626Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"config = {\"persist_directory\":None,\n          \"load_in_8bit\":False,\n          \"embedding\" : EMB_SBERT_MPNET_BASE,\n          \"llm\":LLM_FLAN_T5_BASE,\n          }","metadata":{"execution":{"iopub.status.busy":"2023-09-25T07:48:47.369025Z","iopub.execute_input":"2023-09-25T07:48:47.369479Z","iopub.status.idle":"2023-09-25T07:48:47.374828Z","shell.execute_reply.started":"2023-09-25T07:48:47.369443Z","shell.execute_reply":"2023-09-25T07:48:47.373892Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_sbert_mpnet():\n        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n        return HuggingFaceEmbeddings(model_name=EMB_SBERT_MPNET_BASE, model_kwargs={\"device\": device})\n\n\ndef create_flan_t5_base(load_in_8bit=False):\n        # Wrap it in HF pipeline for use with LangChain\n        model=\"google/flan-t5-base\"\n        tokenizer = AutoTokenizer.from_pretrained(model)\n        return pipeline(\n            task=\"text2text-generation\",\n            model=model,\n            tokenizer = tokenizer,\n            max_new_tokens=100,\n            model_kwargs={\"device_map\": \"auto\", \"load_in_8bit\": load_in_8bit, \"max_length\": 512, \"temperature\": 0.}\n        )\n\n\n\nif config[\"embedding\"] == EMB_SBERT_MPNET_BASE:\n    embedding = create_sbert_mpnet()\nload_in_8bit = config[\"load_in_8bit\"]\nif config[\"llm\"] == LLM_FLAN_T5_BASE:\n    llm = create_flan_t5_base(load_in_8bit=load_in_8bit)","metadata":{"execution":{"iopub.status.busy":"2023-09-25T07:48:54.578363Z","iopub.execute_input":"2023-09-25T07:48:54.578797Z","iopub.status.idle":"2023-09-25T07:49:06.13519Z","shell.execute_reply.started":"2023-09-25T07:48:54.578764Z","shell.execute_reply":"2023-09-25T07:49:06.134065Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load the pdf\npdf_path = \"/kaggle/input/dataset/Intellylabs_Data set.pdf\"\nloader = PDFPlumberLoader(pdf_path)\ndocuments = loader.load()\n\n# Split documents and create text snippets\ntext_splitter = CharacterTextSplitter(chunk_size=100, chunk_overlap=0)\ntexts = text_splitter.split_documents(documents)\ntext_splitter = TokenTextSplitter(chunk_size=1000, chunk_overlap=10, encoding_name=\"cl100k_base\")  # This the encoding for text-embedding-ada-002\ntexts = text_splitter.split_documents(texts)\n\npersist_directory = config[\"persist_directory\"]\nvectordb = Chroma.from_documents(documents=texts, embedding=embedding, persist_directory=persist_directory)","metadata":{"execution":{"iopub.status.busy":"2023-09-25T07:52:49.789544Z","iopub.execute_input":"2023-09-25T07:52:49.789977Z","iopub.status.idle":"2023-09-25T07:53:32.224865Z","shell.execute_reply.started":"2023-09-25T07:52:49.789942Z","shell.execute_reply":"2023-09-25T07:53:32.223601Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"hf_llm = HuggingFacePipeline(pipeline=llm)\nretriever = vectordb.as_retriever(search_kwargs={\"k\":4})\nqa = RetrievalQA.from_chain_type(llm=hf_llm, chain_type=\"stuff\",retriever=retriever)\n\n# Defining a default prompt for flan models\nif config[\"llm\"] == LLM_FLAN_T5_SMALL or config[\"llm\"] == LLM_FLAN_T5_BASE or config[\"llm\"] == LLM_FLAN_T5_LARGE:\n    question_t5_template = \"\"\"\n    context: {context}\n    question: {question}\n    answer: \n    \"\"\"\n    QUESTION_T5_PROMPT = PromptTemplate(\n        template=question_t5_template, input_variables=[\"context\", \"question\"]\n    )\n    qa.combine_documents_chain.llm_chain.prompt = QUESTION_T5_PROMPT","metadata":{"execution":{"iopub.status.busy":"2023-09-25T07:53:32.226794Z","iopub.execute_input":"2023-09-25T07:53:32.227092Z","iopub.status.idle":"2023-09-25T07:53:32.236062Z","shell.execute_reply.started":"2023-09-25T07:53:32.227065Z","shell.execute_reply":"2023-09-25T07:53:32.234923Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"question = \"Types of impacts of climate change?\"\nqa.combine_documents_chain.verbose = True\nqa.return_source_documents = True\nqa({\"query\":question,})\n","metadata":{"execution":{"iopub.status.busy":"2023-09-25T07:53:32.237241Z","iopub.execute_input":"2023-09-25T07:53:32.237507Z","iopub.status.idle":"2023-09-25T07:53:34.670298Z","shell.execute_reply.started":"2023-09-25T07:53:32.237483Z","shell.execute_reply":"2023-09-25T07:53:34.669113Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"****Experimentation for Streamlit app****","metadata":{}},{"cell_type":"code","source":"from langchain.document_loaders import PDFPlumberLoader\nfrom langchain.text_splitter import CharacterTextSplitter, TokenTextSplitter\nfrom transformers import pipeline\nfrom langchain.prompts import PromptTemplate\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain.vectorstores import Chroma\nfrom langchain.chains import RetrievalQA\nfrom langchain import HuggingFacePipeline\nfrom langchain.embeddings import HuggingFaceInstructEmbeddings, HuggingFaceEmbeddings\nfrom langchain.embeddings.openai import OpenAIEmbeddings\nfrom langchain.llms import OpenAI\nfrom constants import *\nfrom transformers import AutoTokenizer\nimport torch\nimport os\nimport re\n\nclass PdfQA:\n    def __init__(self,config:dict = {}):\n        self.config = config\n        self.embedding = None\n        self.vectordb = None\n        self.llm = None\n        self.qa = None\n        self.retriever = None\n\n    # The following class methods are useful to create global GPU model instances\n    # This way we don't need to reload models in an interactive app,\n    # and the same model instance can be used across multiple user sessions\n    @classmethod\n    def create_instructor_xl(cls):\n        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n        return HuggingFaceInstructEmbeddings(model_name=EMB_INSTRUCTOR_XL, model_kwargs={\"device\": device})\n    \n    @classmethod\n    def create_sbert_mpnet(cls):\n        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n        return HuggingFaceEmbeddings(model_name=EMB_SBERT_MPNET_BASE, model_kwargs={\"device\": device})    \n    \n    @classmethod\n    def create_flan_t5_xxl(cls, load_in_8bit=False):\n        # Local flan-t5-xxl with 8-bit quantization for inference\n        # Wrap it in HF pipeline for use with LangChain\n        return pipeline(\n            task=\"text2text-generation\",\n            model=\"google/flan-t5-xxl\",\n            max_new_tokens=200,\n            model_kwargs={\"device_map\": \"auto\", \"load_in_8bit\": load_in_8bit, \"max_length\": 512, \"temperature\": 0.}\n        )\n    @classmethod\n    def create_flan_t5_xl(cls, load_in_8bit=False):\n        return pipeline(\n            task=\"text2text-generation\",\n            model=\"google/flan-t5-xl\",\n            max_new_tokens=200,\n            model_kwargs={\"device_map\": \"auto\", \"load_in_8bit\": load_in_8bit, \"max_length\": 512, \"temperature\": 0.}\n        )\n    \n    @classmethod\n    def create_flan_t5_small(cls, load_in_8bit=False):\n        # Local flan-t5-small for inference\n        # Wrap it in HF pipeline for use with LangChain\n        model=\"google/flan-t5-small\"\n        tokenizer = AutoTokenizer.from_pretrained(model)\n        return pipeline(\n            task=\"text2text-generation\",\n            model=model,\n            tokenizer = tokenizer,\n            max_new_tokens=100,\n            model_kwargs={\"device_map\": \"auto\", \"load_in_8bit\": load_in_8bit, \"max_length\": 512, \"temperature\": 0.}\n        )\n    @classmethod\n    def create_flan_t5_base(cls, load_in_8bit=False):\n        # Wrap it in HF pipeline for use with LangChain\n        model=\"google/flan-t5-base\"\n        tokenizer = AutoTokenizer.from_pretrained(model)\n        return pipeline(\n            task=\"text2text-generation\",\n            model=model,\n            tokenizer = tokenizer,\n            max_new_tokens=100,\n            model_kwargs={\"device_map\": \"auto\", \"load_in_8bit\": load_in_8bit, \"max_length\": 512, \"temperature\": 0.}\n        )\n    @classmethod\n    def create_flan_t5_large(cls, load_in_8bit=False):\n        # Wrap it in HF pipeline for use with LangChain\n        model=\"google/flan-t5-large\"\n        tokenizer = AutoTokenizer.from_pretrained(model)\n        return pipeline(\n            task=\"text2text-generation\",\n            model=model,\n            tokenizer = tokenizer,\n            max_new_tokens=100,\n            model_kwargs={\"device_map\": \"auto\", \"load_in_8bit\": load_in_8bit, \"max_length\": 512, \"temperature\": 0.}\n        )\n    @classmethod\n    def create_fastchat_t5_xl(cls, load_in_8bit=False):\n        return pipeline(\n            task=\"text2text-generation\",\n            model = \"lmsys/fastchat-t5-3b-v1.0\",\n            max_new_tokens=100,\n            model_kwargs={\"device_map\": \"auto\", \"load_in_8bit\": load_in_8bit, \"max_length\": 512, \"temperature\": 0.}\n        )\n    \n    @classmethod\n    def create_falcon_instruct_small(cls, load_in_8bit=False):\n        model = \"tiiuae/falcon-7b-instruct\"\n\n        tokenizer = AutoTokenizer.from_pretrained(model)\n        hf_pipeline = pipeline(\n                task=\"text-generation\",\n                model = model,\n                tokenizer = tokenizer,\n                trust_remote_code = True,\n                max_new_tokens=100,\n                model_kwargs={\n                    \"device_map\": \"auto\", \n                    \"load_in_8bit\": load_in_8bit, \n                    \"max_length\": 512, \n                    \"temperature\": 0.01,\n                    \"torch_dtype\":torch.bfloat16,\n                    }\n            )\n        return hf_pipeline\n    \n    def init_embeddings(self) -> None:\n        # OpenAI ada embeddings API\n        if self.config[\"embedding\"] == EMB_OPENAI_ADA:\n            self.embedding = OpenAIEmbeddings()\n        elif self.config[\"embedding\"] == EMB_INSTRUCTOR_XL:\n            # Local INSTRUCTOR-XL embeddings\n            if self.embedding is None:\n                self.embedding = PdfQA.create_instructor_xl()\n        elif self.config[\"embedding\"] == EMB_SBERT_MPNET_BASE:\n            ## this is for SBERT\n            if self.embedding is None:\n                self.embedding = PdfQA.create_sbert_mpnet()\n        else:\n            self.embedding = None ## DuckDb uses sbert embeddings\n            # raise ValueError(\"Invalid config\")\n\n    def init_models(self) -> None:\n        \"\"\" Initialize LLM models based on config \"\"\"\n        load_in_8bit = self.config.get(\"load_in_8bit\",False)\n        # OpenAI GPT 3.5 API\n        if self.config[\"llm\"] == LLM_OPENAI_GPT35:\n            # OpenAI GPT 3.5 API\n            pass\n        elif self.config[\"llm\"] == LLM_FLAN_T5_SMALL:\n            if self.llm is None:\n                self.llm = PdfQA.create_flan_t5_small(load_in_8bit=load_in_8bit)\n        elif self.config[\"llm\"] == LLM_FLAN_T5_BASE:\n            if self.llm is None:\n                self.llm = PdfQA.create_flan_t5_base(load_in_8bit=load_in_8bit)\n        elif self.config[\"llm\"] == LLM_FLAN_T5_LARGE:\n            if self.llm is None:\n                self.llm = PdfQA.create_flan_t5_large(load_in_8bit=load_in_8bit)\n        elif self.config[\"llm\"] == LLM_FLAN_T5_XL:\n            if self.llm is None:\n                self.llm = PdfQA.create_flan_t5_xl(load_in_8bit=load_in_8bit)\n        elif self.config[\"llm\"] == LLM_FLAN_T5_XXL:\n            if self.llm is None:\n                self.llm = PdfQA.create_flan_t5_xxl(load_in_8bit=load_in_8bit)\n        elif self.config[\"llm\"] == LLM_FASTCHAT_T5_XL:\n            if self.llm is None:\n                self.llm = PdfQA.create_fastchat_t5_xl(load_in_8bit=load_in_8bit)\n        elif self.config[\"llm\"] == LLM_FALCON_SMALL:\n            if self.llm is None:\n                self.llm = PdfQA.create_falcon_instruct_small(load_in_8bit=load_in_8bit)\n        \n        else:\n            raise ValueError(\"Invalid config\")        \n    def vector_db_pdf(self) -> None:\n        \"\"\"\n        creates vector db for the embeddings and persists them or loads a vector db from the persist directory\n        \"\"\"\n        pdf_path = self.config.get(\"pdf_path\",None)\n        persist_directory = self.config.get(\"persist_directory\",None)\n        if persist_directory and os.path.exists(persist_directory):\n            ## Load from the persist db\n            self.vectordb = Chroma(persist_directory=persist_directory, embedding_function=self.embedding)\n        elif pdf_path and os.path.exists(pdf_path):\n            ## 1. Extract the documents\n            loader = PDFPlumberLoader(pdf_path)\n            documents = loader.load()\n            ## 2. Split the texts\n            text_splitter = CharacterTextSplitter(chunk_size=100, chunk_overlap=0)\n            texts = text_splitter.split_documents(documents)\n            # text_splitter = TokenTextSplitter(chunk_size=100, chunk_overlap=10, encoding_name=\"cl100k_base\")  # This the encoding for text-embedding-ada-002\n            text_splitter = TokenTextSplitter(chunk_size=100, chunk_overlap=10)  # This the encoding for text-embedding-ada-002\n            texts = text_splitter.split_documents(texts)\n\n            ## 3. Create Embeddings and add to chroma store\n            ##TODO: Validate if self.embedding is not None\n            self.vectordb = Chroma.from_documents(documents=texts, embedding=self.embedding, persist_directory=persist_directory)\n        else:\n            raise ValueError(\"NO PDF found\")\n\n    def retreival_qa_chain(self):\n        \"\"\"\n        Creates retrieval qa chain using vectordb as retrivar and LLM to complete the prompt\n        \"\"\"\n        ##TODO: Use custom prompt\n        self.retriever = self.vectordb.as_retriever(search_kwargs={\"k\":3})\n        \n        if self.config[\"llm\"] == LLM_OPENAI_GPT35:\n          # Use ChatGPT API\n          self.qa = RetrievalQA.from_chain_type(llm=OpenAI(model_name=LLM_OPENAI_GPT35, temperature=0.), chain_type=\"stuff\",\\\n                                      retriever=self.vectordb.as_retriever(search_kwargs={\"k\":3}))\n        else:\n            hf_llm = HuggingFacePipeline(pipeline=self.llm,model_id=self.config[\"llm\"])\n\n            self.qa = RetrievalQA.from_chain_type(llm=hf_llm, chain_type=\"stuff\",retriever=self.retriever)\n            if self.config[\"llm\"] == LLM_FLAN_T5_SMALL or self.config[\"llm\"] == LLM_FLAN_T5_BASE or self.config[\"llm\"] == LLM_FLAN_T5_LARGE:\n                question_t5_template = \"\"\"\n                context: {context}\n                question: {question}\n                answer: \n                \"\"\"\n                QUESTION_T5_PROMPT = PromptTemplate(\n                    template=question_t5_template, input_variables=[\"context\", \"question\"]\n                )\n                self.qa.combine_documents_chain.llm_chain.prompt = QUESTION_T5_PROMPT\n            self.qa.combine_documents_chain.verbose = True\n            self.qa.return_source_documents = True\n    def answer_query(self,question:str) ->str:\n        \"\"\"\n        Answer the question\n        \"\"\"\n\n        answer_dict = self.qa({\"query\":question,})\n        print(answer_dict)\n        answer = answer_dict[\"result\"]\n        if self.config[\"llm\"] == LLM_FASTCHAT_T5_XL:\n            answer = self._clean_fastchat_t5_output(answer)\n        return answer\n    def _clean_fastchat_t5_output(self, answer: str) -> str:\n        # Remove <pad> tags, double spaces, trailing newline\n        answer = re.sub(r\"<pad>\\s+\", \"\", answer)\n        answer = re.sub(r\"  \", \" \", answer)\n        answer = re.sub(r\"\\n$\", \"\", answer)\n        return answer","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Constants\nEMB_OPENAI_ADA = \"text-embedding-ada-002\"\nEMB_INSTRUCTOR_XL = \"hkunlp/instructor-xl\"\nEMB_SBERT_MPNET_BASE = \"sentence-transformers/all-mpnet-base-v2\" # Chroma takes care if embeddings are None\nEMB_SBERT_MINILM = \"sentence-transformers/all-MiniLM-L6-v2\" # Chroma takes care if embeddings are None\n\n\nLLM_OPENAI_GPT35 = \"gpt-3.5-turbo\"\nLLM_FLAN_T5_XXL = \"google/flan-t5-xxl\"\nLLM_FLAN_T5_XL = \"google/flan-t5-xl\"\nLLM_FASTCHAT_T5_XL = \"lmsys/fastchat-t5-3b-v1.0\"\nLLM_FLAN_T5_SMALL = \"google/flan-t5-small\"\nLLM_FLAN_T5_BASE = \"google/flan-t5-base\"\nLLM_FLAN_T5_LARGE = \"google/flan-t5-large\"\nLLM_FALCON_SMALL = \"tiiuae/falcon-7b-instruct\"\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Configuration for PDFQA\nconfig = {\"persist_directory\":None,\n          \"load_in_8bit\":False,\n          \"embedding\" : EMB_SBERT_MPNET_BASE,\n          \"llm\":LLM_FLAN_T5_BASE,\n          \"pdf_path\":\"/kaggle/input/dataset/Intellylabs_Data set.pdf\"\n          }\n\n# Initialize PDFQA\npdfqa = PdfQA(config=config)\npdfqa.init_embeddings()\npdfqa.init_models()\n\n# Create Vector DB \npdfqa.vector_db_pdf()\n\n# Set up Retrieval QA Chain\npdfqa.retreival_qa_chain()\n\n# Query the model\nquestion = \"what the reason for financial crisis?\"\npdfqa.answer_query(question)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}